{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "36TWde9UlzAH",
        "outputId": "99a8acc8-1020-4051-b801-1767b95f5b89"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1663227445.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_lock_unlock_module\u001b[0;34m(name)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import copy\n",
        "import re\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import GPT2Tokenizer\n",
        "from torchinfo import summary\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "full_dataset = load_dataset(\"skeskinen/TinyStories-GPT4\", split=\"train\")\n",
        "full_dataset = full_dataset.remove_columns([c for c in full_dataset.column_names if c not in [\"story\", \"features\"]])\n",
        "assert len(full_dataset) == 2745100\n",
        "splits = full_dataset.train_test_split(test_size=10000, seed=42, shuffle=True)\n",
        "train_dataset = splits[\"train\"]\n",
        "test_dataset  = splits[\"test\"]\n",
        "assert len(train_dataset) == 2735100\n",
        "assert len(test_dataset)  == 10000\n",
        "assert train_dataset[0][\"story\"][:33] == \"One day, a little girl named Lily\"\n",
        "assert train_dataset[0][\"features\"] == [\"Dialogue\", \"Conflict\"]\n",
        "\n",
        "#Vocabulary Section\n",
        "\n",
        "PAD_TOKEN = \"[PAD]\"\n",
        "BOS_TOKEN = \"[BOS]\" #Beginning of Story\n",
        "EOS_TOKEN = \"[EOS]\" #End of Story\n",
        "TAG_1_TOKEN = \"[TAG_1]\" #BadEnding\n",
        "TAG_2_TOKEN = \"[TAG_2]\" #Conflict\n",
        "TAG_3_TOKEN = \"[TAG_3]\" #Dialogue\n",
        "TAG_4_TOKEN = \"[TAG_4]\" #Foreshadowing\n",
        "TAG_5_TOKEN = \"[TAG_5]\" #MoralValue\n",
        "TAG_6_TOKEN = \"[TAG_6]\" #Twist\n",
        "tags = {TAG_1_TOKEN: \"BadEnding\", TAG_2_TOKEN: \"Conflict\", TAG_3_TOKEN: \"Dialogue\", TAG_4_TOKEN: \"Foreshadowing\", TAG_5_TOKEN: \"MoralValue\", TAG_6_TOKEN: \"Twist\"}\n",
        "\n",
        "#Define special tokens\n",
        "special_tokens = {\n",
        "    \"pad_token\": \"[PAD]\",\n",
        "    \"bos_token\": \"[BOS]\",\n",
        "    \"eos_token\": \"[EOS]\",\n",
        "    \"additional_special_tokens\": [\"[TAG_1]\", \"[TAG_2]\", \"[TAG_3]\", \"[TAG_4]\", \"[TAG_5]\", \"[TAG_6]\"]\n",
        "}\n",
        "\n",
        "tokenizer.add_special_tokens(special_tokens) #Adding special tokens\n",
        "\n",
        "vocab_size = len(tokenizer)\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "\n",
        "#PREPROCESSING SECTION\n",
        "\n",
        "def get_tags_token(features): #From a list of tags it gets the list of corresponding token IDs\n",
        "  tag_token_ids = []\n",
        "  for tag in tags:\n",
        "    if tags[tag] in features:\n",
        "      tag_token_ids.append(tokenizer.encode(tag)[0])\n",
        "  return tag_token_ids\n",
        "\n",
        "def build_data(dataset, context_length): #Converts stories into tensors of l+1 token-sequences for later context-target splitting.\n",
        "  stories = []\n",
        "  for story in dataset:\n",
        "    encoded = tokenizer.encode(story[\"story\"])\n",
        "    tag_vec = get_tags_token(story[\"features\"])\n",
        "    num_tag = len(tag_vec)\n",
        "    L = context_length + 1 - num_tag\n",
        "    # Create the full encoded sequence with BOS, tags, story, and EOS\n",
        "    story_enc = [tokenizer.encode(PAD_TOKEN)[0]] * (context_length-1-num_tag) + [tokenizer.encode(BOS_TOKEN)[0]] + encoded + [tokenizer.encode(EOS_TOKEN)[0]]\n",
        "    contexts = []\n",
        "    for i in range(len(story_enc) - L + 1):\n",
        "      context = tag_vec + story_enc[i:i+L]\n",
        "      contexts.append(context)\n",
        "    # Only process if there are any valid contexts to create\n",
        "    if contexts:\n",
        "        contexts_story = torch.tensor(contexts, dtype=torch.long)\n",
        "        stories.append(contexts_story)\n",
        "  return stories\n",
        "\n",
        "l = 50 #Context lenght\n",
        "random_seed = 42\n",
        "dim_subset = 100000\n",
        "fraction = (dim_subset + 1) / len(train_dataset)\n",
        "subset = train_dataset.train_test_split(train_size = fraction, seed = random_seed)[\"train\"]\n",
        "dim_data = int(dim_subset*0.9) #split 90/10\n",
        "\n",
        "data_train = build_data(subset.select(range(dim_data)), l) #building the training dataset\n",
        "data_val = build_data(subset.select(range(dim_data, dim_subset)), l) #building the validation dataset\n",
        "\n",
        "#MODEL ARCHITECTURE\n",
        "\n",
        "emb_dim = 256\n",
        "hidden_dim_1 = 768\n",
        "hidden_dim_2 = 1024\n",
        "num_heads = 8\n",
        "\n",
        "encoder_layer_1= nn.TransformerEncoderLayer(\n",
        "    d_model=emb_dim,\n",
        "    nhead=num_heads,\n",
        "    dim_feedforward=hidden_dim_1,\n",
        "    batch_first=True,\n",
        ")\n",
        "\n",
        "encoder_layer_2= nn.TransformerEncoderLayer(\n",
        "    d_model=emb_dim,\n",
        "    nhead=num_heads,\n",
        "    dim_feedforward=hidden_dim_2,\n",
        "    batch_first=True,\n",
        ")\n",
        "\n",
        "class LearnedPositionalEmbedding(nn.Module): #to know in which point of the sequence is the token\n",
        "    \"\"\"\n",
        "    Adds a trainable position vector to each token embedding.\n",
        "    Expects input shape [B, L, D] (batch_first).\n",
        "    \"\"\"\n",
        "    def __init__(self, max_length: int, dim: int):\n",
        "        super().__init__()\n",
        "        self.pos_emb = nn.Embedding(max_length, dim)\n",
        "\n",
        "    #B = batch size, L = sequence lenght, D = embedding dimension\n",
        "\n",
        "    def forward(self, x):                   # x: [B, L, D]\n",
        "        L = x.size(1)\n",
        "        pos = torch.arange(L, device=x.device)          # [L] generates a tensor the tensor of the position in the sequence: [0, 1, 2, ..., L-1]\n",
        "        pos = pos.unsqueeze(0)                          # [1, L] adds the batch dimension\n",
        "        return x + self.pos_emb(pos)                    # adds the info of the position to the embedding vector x\n",
        "\n",
        "class SelectLastToken(nn.Module):\n",
        "    \"\"\"\n",
        "    Keeps only the hidden state of the *final* token in the sequence.\n",
        "    Input  : [B, L, D]\n",
        "    Output : [B, D]\n",
        "    \"\"\"\n",
        "    def forward(self, x):\n",
        "        return x[:, -1, :]\n",
        "\n",
        "padding_index = tokenizer.encode(PAD_TOKEN)[0]\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Embedding(vocab_size, emb_dim, padding_idx=padding_index),\n",
        "    LearnedPositionalEmbedding(l, emb_dim),\n",
        "    nn.TransformerEncoder(encoder_layer_1, num_layers=3),\n",
        "    nn.Dropout(0.1),\n",
        "    nn.TransformerEncoder(encoder_layer_2, num_layers=3),\n",
        "    nn.Dropout(0.1),\n",
        "    nn.TransformerEncoder(encoder_layer_2, num_layers=3),\n",
        "    nn.Dropout(0.1),\n",
        "    nn.TransformerEncoder(encoder_layer_1, num_layers=3),\n",
        "    nn.Dropout(0.1),\n",
        "    SelectLastToken(),\n",
        "    nn.Linear(emb_dim, vocab_size)\n",
        ")\n",
        "\n",
        "summary(model, input_size=(emb_dim, l), dtypes=['torch.IntTensor'], device='cpu')\n",
        "\n",
        "#TRAINING SECTION\n",
        "\n",
        "def train(model, optimizer, data_train, data_val, epochs, device):\n",
        "    model.to(device)\n",
        "    # Data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        data_train, batch_size=1, shuffle=True,\n",
        "    )\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        data_val, batch_size=1, shuffle=False\n",
        "    )\n",
        "    # Initial evaluation\n",
        "    evaluate(model, val_loader, device)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        # Training loop\n",
        "        for context in train_loader:\n",
        "            story = context[0].to(device)\n",
        "            data = story[:, :-1].to(device)\n",
        "            target = story[:, -1].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            # Reshape to (batch*l, vocab_size) and (batch*l)\n",
        "            output = output.reshape(-1, output.size(-1))\n",
        "            target = target.reshape(-1)\n",
        "            loss = loss_fn(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "        train_loss = float(np.mean(train_losses))\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f}\")\n",
        "        # Validation\n",
        "        val_loss = evaluate(model, val_loader, device)\n",
        "\n",
        "\n",
        "def evaluate(model, loader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    with torch.no_grad():\n",
        "        for context in loader:\n",
        "            story = context[0].to(device)\n",
        "            data = story[:, :-1].squeeze().to(device)\n",
        "            target = story[:, -1].squeeze().to(device)\n",
        "            output = model(data)\n",
        "            output = output.reshape(-1, output.size(-1))\n",
        "            target = target.reshape(-1)\n",
        "            total_loss += loss_fn(output, target).item()\n",
        "    average_loss = total_loss / len(loader)\n",
        "    print(f\"Validation Loss: {average_loss:.4f} | Perplexity: {math.exp(average_loss):.2f}\")\n",
        "    return average_loss\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Detect device automatically\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "num_epochs = 10\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "train(model, optimizer, data_train, data_val, epochs=num_epochs, device=device)\n",
        "\n",
        "#TEXT GENERATING SECTION\n",
        "\n",
        "def padding_truncate(seq, context_length):\n",
        "  n = len(seq)\n",
        "  copia = copy.copy(seq)\n",
        "  if n > context_length:\n",
        "    copia = copia[-context_length:]\n",
        "  elif n < context_length:\n",
        "    for i in range(context_length-n):\n",
        "      copia.append(tokenizer.encode(PAD_TOKEN)[0])\n",
        "  return copia\n",
        "\n",
        "\n",
        "def top_p_filtering(logits, top_p = 0.9, filter_value = -float(\"inf\")): #to implement top_p sampling\n",
        "  sorted_logits, sorted_indices = torch.sort(logits, descending = True) #decrescent order\n",
        "  cumulative_prob = torch.cumsum(torch.softmax(sorted_logits, dim = -1), dim = -1)\n",
        "  sorted_indices_to_remove = cumulative_prob > top_p #create a boolean mask\n",
        "  sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
        "  sorted_indices_to_remove[:, 0] = False\n",
        "  indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "  logits[-1][indices_to_remove] = filter_value\n",
        "  return logits\n",
        "\n",
        "\n",
        "def generate_text(model, tags = [], device = None, top_p = 0.9, context_length = l):\n",
        "  if device is None:\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  model.eval()\n",
        "  for tag in tags:\n",
        "    if tag not in [\"BadEnding\", \"Conflict\", \"Dialogue\", \"Foreshadowing\", \"MoralValue\", \"Twist\"]:\n",
        "      return str(\"This tag: \" + tag + \" is invalid. Choose tag among: BadEnding, Conflict, Dialogue, Foreshadowing, MoralValue, Twist\")\n",
        "  tag_vec = get_tags_token(tags)\n",
        "  num_tags = len(tag_vec)\n",
        "  enc = tokenizer.encode(PAD_TOKEN)*(context_length-1-num_tags) + tokenizer.encode(BOS_TOKEN) #create input for the model\n",
        "  eos_token_id = tokenizer.encode(EOS_TOKEN)[0]\n",
        "  while enc[-1] != eos_token_id and len(enc)<350:\n",
        "    x = padding_truncate(enc, context_length - num_tags)\n",
        "    x = tag_vec + x\n",
        "    x = torch.tensor(x).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "      output = model(x)\n",
        "      #add a bias to EOS token that increases with the lenght of the story from 0 to 5\n",
        "      #starts with 0 at 200 tokens, reaching 5 at 250 tokens\n",
        "      len_story = len(enc) - context_length + num_tags\n",
        "      if len_story > 200:\n",
        "        eos_bias = min(5, 0.1 * (len_story - 200))\n",
        "        output[-1][eos_token_id] += eos_bias\n",
        "      logits = top_p_filtering(output, top_p=top_p) #top_p\n",
        "      next_word = torch.multinomial(torch.softmax(logits, -1), 1).item() #predict the next token\n",
        "      enc.append(next_word)\n",
        "  # Decode from the token after the BOS token and before the EOS token (if present)\n",
        "  story = enc[context_length - num_tags : -1 if enc[-1] == eos_token_id else None]\n",
        "  text = tokenizer.decode(story)\n",
        "  print(\"Generating story with\", tags)\n",
        "  print(text) #print instead of return so that we don't see anymore characters like \\n\n",
        "\n",
        "#TEST SECTION\n",
        "\n",
        "def avg_cross_entropy(token_probs):\n",
        "    \"\"\"\n",
        "    Average over all stories of the sum of -log p for every ground-truth token.\n",
        "\n",
        "    token_probs : list[list[float]]\n",
        "        Probabilities assigned to the correct token at each position.\n",
        "    \"\"\"\n",
        "    return np.average([-np.log(story_token_probs).sum() for story_token_probs in token_probs])\n",
        "\n",
        "#Building the test_dataset to have a clear structure\n",
        "test_subset = test_dataset.select(range(100))  # Select first 100 test examples\n",
        "test_data = [\n",
        "    {\"story\": s, \"features\": f}\n",
        "    for s, f in zip(test_subset[\"story\"], test_subset[\"features\"])\n",
        "]\n",
        "\n",
        "#Test the model on the test_dataset with the function provided\n",
        "\n",
        "model.eval()\n",
        "\n",
        "token_probs = []\n",
        "for story in test_data:\n",
        "    tag_vec = get_tags_token(story[\"features\"])\n",
        "    num_tags = len(tag_vec)\n",
        "    true_tokens = tokenizer.encode(story[\"story\"]) + tokenizer.encode(EOS_TOKEN)\n",
        "    input = tokenizer.encode(PAD_TOKEN)*(l-1-num_tags) + tokenizer.encode(BOS_TOKEN)\n",
        "    probs = []\n",
        "    #for each true_token we take the probability generated by the model\n",
        "    for next_token in true_tokens:\n",
        "        x = padding_truncate(input, l - num_tags)\n",
        "        x = tag_vec + x\n",
        "        x = torch.tensor(x).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(x) #predicts the probability of the next_token based on the last l tokens\n",
        "            distrib = torch.softmax(output, -1)\n",
        "            prob = distrib[-1][next_token].item()\n",
        "            probs.append(prob)\n",
        "        input.append(next_token)\n",
        "    token_probs.append(probs)\n",
        "\n",
        "print(f\"Average cross-entropy {avg_cross_entropy(token_probs):.2f} nats/story\")"
      ]
    }
  ]
}